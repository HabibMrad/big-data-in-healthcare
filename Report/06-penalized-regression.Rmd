# Regressione penalizzata su set di variabili di espressione genica

Carichiamo il dataset prendendo solo le covariate di espressione genica, il tempo e l'outcome:
```{r message=FALSE, warning=FALSE}
library("survival")
library("glmnet")

# carico il dataset
dataset <- read.table(
  "../dataset.csv",
  na.strings = ".",
  sep = "\t",
  header = T,
  row.names = NULL
)

# Rimuovo l'id della riga
dataset <- dataset[,-1]

dataset <- dataset[c(1, 2, 8:length(dataset))]
```

## Kaplan-Meier
Stimiamo la probabilità di evento nel tempo utilizzando lo stimatore Kaplan-Meier:
```{r}
fit <- survfit(Surv(time, event) ~ 1, data = dataset)

par(mar = c(4, 4, 2, 2))
plot(
  fit,
  fun = "F",
  xlab = 'Tempo di followup libero da metastasi',
  ylab = 'Probabilità di evento',
  ylim = c(0, 1)
)
```

## Valutiamo la correlazione tra le covariate geniche

```{r,  fig.width = 8, fig.height = 7, warning=FALSE}
library("corrplot")

corrs <- cor(dataset[, 3:72])
a <- apply(corrs, 1, function(x) abs(x) > 0.75 & abs(x) < 1)
# variables with a correlation >0.75 with at least another variable
b <- rowSums(a) > 0
vars <- names(b[b == T])
corrplot(cor(dataset[, vars]), method = "number")
```
Notiamo subito che abbiamo diverse covariate molto correlate fra di loro.

## Valutazione della correlazione tra le covariate geniche e l'outcome
Utilizziamo un modello di Cox univariato per ogni covariata per valutarne l'associazione con l'outcome:
```{r}
library("pander")
vars <- names(dataset[, 3:72])
output <- lapply(vars, function(var) {
  formula    <- as.formula(paste("Surv(time, event) ~ ", var))
  fit.uni    <- coxph(formula, data = dataset)
  beta       <- coef(fit.uni)
  se         <- sqrt(diag(fit.uni$var))
  CI         <- round(exp(confint(fit.uni)), 3)
  round(c(exp(beta), CI, p = 1 - pchisq((beta / se) ^ 2, 1)), 3)
})
results <- as.data.frame(matrix(unlist(output), ncol = 4, byrow = T))
names(results) <- c("HR", "lower95%CI", "upper95%CI", "p")
results$features <- vars
pander(results[results[order(results$p), ]$p <= 0.05, ])
```
In tabella vengono mostrate tutte quelle covariate il cui test è particolarmente significativo (p-value < 0.05).
Per la tabella completa si rimanda all'apposita sezione nell'appendice.
### Valutazione degli errori
Verifichiamo quali covariate presentano una associazione significativa al 5% dopo l'applicazione di metodi per valutare l'errore dovuto a comparazioni multiple.
Valutiamo inizialmente il Family-Wise Error Rate (FWER) utilizzando la procedura di Holm
```{r}
results$q.holm <- p.adjust(results$p, method = "holm")
results[results$q.holm < 0.05, ]
```

Valutiamo poi il False Discovery Rate (FDR) utilizzando la procedura di Benjamini-Hochberg:

```{r}
results$q.BH <- p.adjust(results$p, method = "BH")
results[results$q.BH < 0.05, ]
```

## Regressione penalizzata

Un modello di Cox standard ha come obiettivo la massimizzazioni della partial Log-likelihood:
$$ LPL(\beta|X) = \sum_{j = 1}^{J}\beta'*x_{(j)} - log \Big[\sum_{i \in R(t_{(j)})}e^{\beta'*x_i} \Big] $$

La regressione penalizzata aggiunge un termine di penalizzazione che riduce i coefficienti fino a zero permettendo quindi di rimuovere la maggior parte delle variabili che non influiscono sull'outcome del modello.
$$ LPL(\beta|X, \lambda) =  LPL(\beta|X) - p_\lambda(\beta) $$

### Penalizzazione ridge

Nella penalizzazione ridge il termine di penalizzazione è definito come:
$$ p_\lambda(\beta) = \lambda\sum_{j = 1}^{p}\beta_j^2  $$

```{r}
X <- model.matrix(~ ., subset(dataset, select = -c(time, event)))

ridge.pen <-
  glmnet(
    x = X[,-1],
    y = Surv(dataset$time, dataset$event),
    family = "cox",
    alpha = 0,
    nlambda = 100
  )

lambdas <- ridge.pen$lambda

par(mar = c(4, 4, 5, 2))
plot(
  lambdas,
  lambdas,
  type = "n",
  xlab = bquote(beta),
  xlim = range(ridge.pen$beta),
  ylab = bquote(lambda),
  ylim = rev(range(lambdas)),
  yaxs = "i",
  log = "y"
)
abline(v = 0, lwd = 4)

for (i in 1:nrow(ridge.pen$beta))
  lines(ridge.pen$beta[i,], lambdas, col = i, lwd = 2)

mtext(
  rownames(ridge.pen$beta),
  3,
  at = ridge.pen$beta[, 100],
  line = .2,
  col = 1:nrow(ridge.pen$beta),
  font = 2,
  las = 3,
  adj = 0,
  cex = .75
)
```
La particolarità della penalizzazione ridge è quella di far avvicinare tutti i coefficienti a zero senza però annullarli. \'E un metodo che infatti non viene quasi mai utilizzato per eseguire feature selection.
Dall'immagine vediamo ovviamente che più aumenta $\lambda$, più tutti i coefficienti vanno verso lo zero. 

### Penalizzazione Lasso
Nella penalizzazione Lasso il termine di penalizzazione è definito come:
$$ p_\lambda(\beta) = \lambda\sum_{j = 1}^{p}|\beta_j|  $$
```{r}
X <- model.matrix( ~ ., subset(dataset, select = -c(time, event)))

lasso.pen <-
  glmnet(
    x = X[,-1],
    y = Surv(dataset$time, dataset$event),
    family = "cox",
    alpha = 1,
    nlambda = 100
  )

lambdas <- lasso.pen$lambda

par(mar = c(4, 4, 5, 2))
plot(
  lambdas,
  lambdas,
  type = "n",
  xlab = bquote(beta),
  xlim = range(lasso.pen$beta),
  ylab = bquote(lambda),
  ylim = rev(range(lambdas)),
  yaxs = "i",
  log = "y"
)
abline(v = 0, lwd = 4)

for (i in 1:nrow(lasso.pen$beta))
  lines(lasso.pen$beta[i,], lambdas, col = i, lwd = 2)

mtext(
  rownames(lasso.pen$beta),
  3,
  at = lasso.pen$beta[, length(lambdas)],
  line = .2,
  col = 1:nrow(lasso.pen$beta),
  font = 2,
  las = 3,
  adj = 0,
  cex = .75
)
```
Anche in questo caso vediamo che più il valore di $\lambda$ aumenta più le variabili vengono ristrette fino però ad azzerarsi completamente.

## Lambda ottimale per la penalizzazione Lasso
Utilizziamo la 10 fold cross validation per addestrare il modello:
```{r warning=FALSE}
X <- model.matrix( ~ ., subset(dataset, select = -c(time, event)))
set.seed(16052019)

cv.lasso <-
  cv.glmnet(
    x = X[, -1],
    y = Surv(dataset$time, dataset$event),
    family = 'cox',
    nfold = 10,
    alpha = 1
  )

plot(
  cv.lasso$lambda,
  cv.lasso$cvm,
  type = "l",
  lwd = 3,
  xlab = bquote(lambda),
  ylab = "Partial Likelihood Deviance"
)

points(cv.lasso$lambda[which.min(cv.lasso$cvm)],
       min(cv.lasso$cvm),
       pch = 16,
       col = 2)

print(cv.lasso$lambda[which.min(cv.lasso$cvm)])
```
Il valore di $\lambda = 0.02444646$ ottimale è quello che minimizza il mean cross-validated error.
Mostriamo quindi quali variabili sarebbero selezionate dalla penalizzazione Lasso utilizzando questo valore di $\lambda$.
```{r}
opt.lambda <- cv.lasso$lambda[which.min(cv.lasso$cvm)]
opt.lambda.coef <- as.numeric(coef(cv.lasso, s = opt.lambda))

# save coefficients (as HRs):
lasso.HR <- round(exp(opt.lambda.coef), 3)

#selected features:
rownames(coef(cv.lasso))[opt.lambda.coef != 0]
```

## Penalizzazione Elastic Net
Fit a multiple Cox model with Elastic Net penalty (set alpha=0.5) using cross validation to find the optimal value of the shrinkage parameter. Find which features would be selected using the optimal lambda.
```{r}
X <- model.matrix( ~ ., subset(dataset, select = -c(time, event)))
set.seed(16052019)

cv.el <-
  cv.glmnet(
    x = X[,-1],
    y = Surv(dataset$time, dataset$event),
    family = 'cox',
    nfold = 10,
    alpha = 0.5
  )

plot(
  cv.el$lambda,
  cv.el$cvm,
  type = "l",
  lwd = 3,
  xlab = bquote(lambda),
  ylab = "Partial Likelihood Deviance"
)
points(cv.el$lambda[which.min(cv.el$cvm)], min(cv.el$cvm), pch = 16, col = 2)
```
```{r}
opt.lambda <- cv.el$lambda[which.min(cv.el$cvm)]
opt.lambda.coef <- as.numeric(coef(cv.el, s = opt.lambda))

#selected features:
rownames(coef(cv.el))[opt.lambda.coef != 0]

# save coefficients (as HRs):
el.HR <- round(exp(opt.lambda.coef), 3)
```

#### Fit a multiple Cox model with Adaptive Lasso penalty (use the inverse of the absolute value of the coefficients from a model with a Lasso penalty as features-specific penalties) using cross validation to find the optimal value of the shrinkage parameter. Find which features would be selected using the optimal lambda.

```{r}
X <- model.matrix( ~ ., subset(dataset, select = -c(time, event)))
set.seed(16052019)

cv.lasso1 <-
  cv.glmnet(
    x = X[,-1],
    y = Surv(dataset$time, dataset$event),
    family = 'cox',
    nfold = 10,
    alpha = 1
  )

best.coef <-
  as.numeric(coef(cv.lasso1, s = cv.lasso1$lambda[which.min(cv.lasso1$cvm)]))
round(best.coef, 3)
```

```{r}
set.seed(16052019)
cv.adlasso <-
  cv.glmnet(
    x = X[,-1],
    y = Surv(dataset$time, dataset$event),
    family = 'cox',
    penalty.factor = 1 / abs(best.coef),
    nfold = 10,
    alpha = 1
  )

opt.lambda <- cv.adlasso$lambda[which.min(cv.adlasso$cvm)]
opt.lambda.coef <- as.numeric(coef(cv.adlasso, s = opt.lambda))

# save coefficients (as HRs):
adlasso.HR <- round(exp(opt.lambda.coef), 3)

# selected features:
rownames(coef(cv.adlasso))[opt.lambda.coef!=0]
```
#### Compare the selected features and the estimated association parameters (HRs) according to all penalized models and the univariate analysis
```{r}
cbind(
  features = results$features,
  univariate.HR = results$HR,
  lasso.HR,
  el.HR,
  adlasso.HR
)
```